protocols:
  http:
    ports: [80, 443, 5000, 8000]
    paths:
      - pattern: "/v1/*"
        extract:
          - json_path: "usage.total_tokens"
            metric: "tokens_used"
          - header: "x-response-time"
            metric: "latency_ms"

  grpc:
    services:
      - "inference.ModelService"
      - "prediction.*"

outcome_detection:
  success_conditions:
    - protocol: http
      status_codes: [200, 201]
      content_match: '"status":"success"'
    
    - protocol: grpc
      status_codes: [0]  # gRPC OK

behavior_profiles:
  - name: "high_latency_success"
    condition: "latency_ms > 1000 AND status == 200"
    severity: "warning"